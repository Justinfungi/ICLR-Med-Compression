"""
ACDC Dataset Loader
ç”¨äºåŠ è½½å’Œå¤„ç†ACDCå¿ƒè„MRIæ•°æ®é›†çš„PyTorchæ•°æ®åŠ è½½å™¨

æ”¯æŒåŠŸèƒ½:
- 3D/4Dæ•°æ®åŠ è½½  
- å…³é”®æ—¶ç›¸æ•°æ®æå–
- å¤šç§æ•°æ®æ ¼å¼æ”¯æŒ
- æ•°æ®é¢„å¤„ç†å’Œå¢å¼º
- ç–¾ç—…åˆ†ç±»æ ‡ç­¾å¤„ç†

ä¸»è¦ç±»:
- ACDCDataset: æ ¸å¿ƒæ•°æ®é›†ç±»
- ACDCDataModule: ä¾¿æ·çš„æ•°æ®æ¨¡å—åŒ…è£…å™¨
"""

import os
import re
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union, Any
import SimpleITK as sitk
import pandas as pd
from scipy import ndimage
import warnings

warnings.filterwarnings("ignore", category=UserWarning, module="SimpleITK")

class ACDCDataset(Dataset):
    """
    ACDCå¿ƒè„MRIæ•°æ®é›†åŠ è½½å™¨
    
    æ”¯æŒå¤šç§åŠ è½½æ¨¡å¼:
    - 3Då•å¸§æ•°æ® (ED/ESå…³é”®æ—¶ç›¸)
    - 4Dæ—¶åºæ•°æ® (å®Œæ•´å¿ƒè·³å‘¨æœŸ)
    - åˆ†å‰²æ ‡æ³¨æ•°æ®
    - æ‚£è€…å…ƒä¿¡æ¯
    """
    
    # ç–¾ç—…åˆ†ç±»æ˜ å°„
    DISEASE_MAPPING = {
        'NOR': 0,   # æ­£å¸¸
        'DCM': 1,   # æ‰©å¼ æ€§å¿ƒè‚Œç—…
        'HCM': 2,   # è‚¥åšæ€§å¿ƒè‚Œç—…  
        'ARV': 3,   # å¼‚å¸¸å³å¿ƒå®¤
        'MINF': 4   # å¿ƒæ¢—åæ”¹å˜
    }
    
    # åˆ†å‰²æ ‡ç­¾æ˜ å°„
    SEGMENTATION_LABELS = {
        'background': 0,      # èƒŒæ™¯
        'rv_cavity': 1,       # å³å¿ƒå®¤è…”
        'lv_myocardium': 2,   # å·¦å¿ƒå®¤å¿ƒè‚Œ
        'lv_cavity': 3        # å·¦å¿ƒå®¤è…”
    }
    
    def __init__(
        self, 
        data_root: Union[str, Path],
        split: str = 'training',
        mode: str = '3d_keyframes',
        load_segmentation: bool = True,
        transform: Optional[callable] = None,
        target_spacing: Optional[Tuple[float, ...]] = None,
        normalize: bool = True,
        cache_data: bool = False
    ):
        """
        åˆå§‹åŒ–ACDCæ•°æ®é›†
        
        Args:
            data_root: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
            split: æ•°æ®é›†åˆ†å‰² ('training', 'testing')
            mode: åŠ è½½æ¨¡å¼ ('3d_keyframes', '4d_sequence', 'ed_only', 'es_only')
            load_segmentation: æ˜¯å¦åŠ è½½åˆ†å‰²æ ‡æ³¨
            transform: æ•°æ®å˜æ¢å‡½æ•°
            target_spacing: ç›®æ ‡ä½“ç´ é—´è· (x, y, z)
            normalize: æ˜¯å¦è¿›è¡Œå¼ºåº¦å½’ä¸€åŒ–
            cache_data: æ˜¯å¦ç¼“å­˜æ•°æ®åˆ°å†…å­˜
        """
        self.data_root = Path(data_root)
        self.split = split
        self.mode = mode
        self.load_segmentation = load_segmentation
        self.transform = transform
        self.target_spacing = target_spacing
        self.normalize = normalize
        self.cache_data = cache_data
        
        # æ•°æ®ç¼“å­˜
        self._cache = {} if cache_data else None
        
        # æ‰«ææ•°æ®æ–‡ä»¶
        self.patient_list = self._scan_patients()
        self.patient_info = self._load_patient_info()
        
        print(f"âœ… åŠ è½½ACDC {split}æ•°æ®é›†: {len(self.patient_list)}ä¾‹æ‚£è€…")
        print(f"ğŸ“Š åŠ è½½æ¨¡å¼: {mode}")
        print(f"ğŸ¯ åˆ†å‰²æ ‡æ³¨: {'å¼€å¯' if load_segmentation else 'å…³é—­'}")
    
    def _scan_patients(self) -> List[str]:
        """æ‰«ææ‚£è€…æ–‡ä»¶å¤¹"""
        split_dir = self.data_root / self.split
        if not split_dir.exists():
            raise FileNotFoundError(f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {split_dir}")
        
        # è·å–æ‰€æœ‰æ‚£è€…æ–‡ä»¶å¤¹
        patient_dirs = [d for d in split_dir.iterdir() if d.is_dir() and d.name.startswith('patient')]
        patient_list = sorted([d.name for d in patient_dirs])
        
        return patient_list
    
    def _load_patient_info(self) -> Dict[str, Dict]:
        """åŠ è½½æ‰€æœ‰æ‚£è€…çš„åŸºæœ¬ä¿¡æ¯"""
        patient_info = {}
        
        for patient_id in self.patient_list:
            info_file = self.data_root / self.split / patient_id / 'Info.cfg'
            if info_file.exists():
                info = self._parse_info_file(info_file)
                patient_info[patient_id] = info
            else:
                warnings.warn(f"Info.cfgæ–‡ä»¶ä¸å­˜åœ¨: {patient_id}")
                patient_info[patient_id] = {}
        
        return patient_info
    
    def _parse_info_file(self, info_file: Path) -> Dict:
        """è§£ææ‚£è€…ä¿¡æ¯æ–‡ä»¶"""
        info = {}
        with open(info_file, 'r') as f:
            for line in f:
                if ':' in line:
                    key, value = line.strip().split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    
                    # ç±»å‹è½¬æ¢
                    if key in ['ED', 'ES', 'NbFrame']:
                        info[key] = int(value)
                    elif key in ['Height', 'Weight']:
                        info[key] = float(value)
                    else:
                        info[key] = value
        
        return info
    
    def _load_nifti(self, filepath: Path) -> Tuple[np.ndarray, Dict]:
        """åŠ è½½NIfTIæ–‡ä»¶"""
        if not filepath.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        
        # ä½¿ç”¨SimpleITKåŠ è½½
        image = sitk.ReadImage(str(filepath))
        
        # æå–å›¾åƒæ•°ç»„
        image_array = sitk.GetArrayFromImage(image)
        
        # æå–å…ƒä¿¡æ¯
        metadata = {
            'origin': image.GetOrigin(),
            'spacing': image.GetSpacing(),
            'direction': image.GetDirection(),
            'size': image.GetSize()
        }
        
        return image_array, metadata
    
    def _resample_image(self, image: np.ndarray, original_spacing: Tuple, target_spacing: Tuple) -> np.ndarray:
        """é‡é‡‡æ ·å›¾åƒåˆ°ç›®æ ‡åˆ†è¾¨ç‡"""
        if len(original_spacing) != len(target_spacing):
            raise ValueError(f"spacingç»´åº¦ä¸åŒ¹é…: {len(original_spacing)} vs {len(target_spacing)}")
        
        # è®¡ç®—ç¼©æ”¾å› å­
        zoom_factors = [orig / target for orig, target in zip(original_spacing, target_spacing)]
        
        # æ‰§è¡Œé‡é‡‡æ ·
        if image.dtype == np.int16 or image.dtype == np.int32:
            # å¯¹äºæ ‡ç­¾æ•°æ®ä½¿ç”¨æœ€è¿‘é‚»æ’å€¼
            resampled = ndimage.zoom(image, zoom_factors, order=0)
        else:
            # å¯¹äºå›¾åƒæ•°æ®ä½¿ç”¨çº¿æ€§æ’å€¼
            resampled = ndimage.zoom(image, zoom_factors, order=1)
        
        return resampled
    
    def _normalize_intensity(self, image: np.ndarray, method: str = 'z_score') -> np.ndarray:
        """å¼ºåº¦å½’ä¸€åŒ–"""
        if method == 'z_score':
            # Z-scoreå½’ä¸€åŒ–
            mean = np.mean(image)
            std = np.std(image)
            if std > 0:
                normalized = (image - mean) / std
            else:
                normalized = image - mean
        elif method == 'min_max':
            # Min-Maxå½’ä¸€åŒ–åˆ°[0,1]
            min_val = np.min(image)
            max_val = np.max(image)
            if max_val > min_val:
                normalized = (image - min_val) / (max_val - min_val)
            else:
                normalized = np.zeros_like(image)
        else:
            normalized = image
        
        return normalized.astype(np.float32)
    
    def _get_patient_files(self, patient_id: str) -> Dict[str, Path]:
        """è·å–æ‚£è€…çš„æ‰€æœ‰æ–‡ä»¶è·¯å¾„"""
        patient_dir = self.data_root / self.split / patient_id
        files = {}
        
        # 4Dåºåˆ—æ–‡ä»¶
        files['4d'] = patient_dir / f"{patient_id}_4d.nii.gz"
        
        # å…³é”®æ—¶ç›¸æ–‡ä»¶
        info = self.patient_info.get(patient_id, {})
        ed_frame = info.get('ED', 1)
        es_frame = info.get('ES', 14)
        
        files['ed_image'] = patient_dir / f"{patient_id}_frame{ed_frame:02d}.nii.gz"
        files['es_image'] = patient_dir / f"{patient_id}_frame{es_frame:02d}.nii.gz"
        
        if self.load_segmentation:
            files['ed_seg'] = patient_dir / f"{patient_id}_frame{ed_frame:02d}_gt.nii.gz"
            files['es_seg'] = patient_dir / f"{patient_id}_frame{es_frame:02d}_gt.nii.gz"
        
        return files
    
    def _load_patient_data(self, patient_id: str) -> Dict[str, Any]:
        """åŠ è½½å•ä¸ªæ‚£è€…çš„æ•°æ®"""
        # æ£€æŸ¥ç¼“å­˜
        if self._cache is not None and patient_id in self._cache:
            return self._cache[patient_id]
        
        files = self._get_patient_files(patient_id)
        data = {'patient_id': patient_id}
        
        # åŠ è½½ä¸åŒæ¨¡å¼çš„æ•°æ®
        if self.mode == '4d_sequence':
            # åŠ è½½4Dæ—¶åºæ•°æ®
            if files['4d'].exists():
                image_4d, metadata = self._load_nifti(files['4d'])
                data['image'] = image_4d
                data['metadata'] = metadata
            else:
                raise FileNotFoundError(f"4Dæ–‡ä»¶ä¸å­˜åœ¨: {files['4d']}")
        
        elif self.mode == '3d_keyframes':
            # åŠ è½½EDå’ŒESå…³é”®å¸§
            images, segmentations = [], []
            
            for phase, img_file in [('ed', files['ed_image']), ('es', files['es_image'])]:
                if img_file.exists():
                    image, metadata = self._load_nifti(img_file)
                    images.append(image)
                    
                    # åŠ è½½åˆ†å‰²æ ‡æ³¨
                    if self.load_segmentation:
                        seg_file = files[f'{phase}_seg']
                        if seg_file.exists():
                            seg, _ = self._load_nifti(seg_file)
                            segmentations.append(seg)
                        else:
                            warnings.warn(f"åˆ†å‰²æ–‡ä»¶ä¸å­˜åœ¨: {seg_file}")
                            segmentations.append(np.zeros_like(image))
            
            data['images'] = np.stack(images) if images else None
            data['segmentations'] = np.stack(segmentations) if segmentations else None
            data['metadata'] = metadata if 'metadata' in locals() else {}
        
        elif self.mode in ['ed_only', 'es_only']:
            # åªåŠ è½½EDæˆ–ES
            phase = self.mode.split('_')[0]
            img_file = files[f'{phase}_image']
            
            if img_file.exists():
                image, metadata = self._load_nifti(img_file)
                data['image'] = image
                data['metadata'] = metadata
                
                if self.load_segmentation:
                    seg_file = files[f'{phase}_seg']
                    if seg_file.exists():
                        seg, _ = self._load_nifti(seg_file)
                        data['segmentation'] = seg
        
        # æ•°æ®é¢„å¤„ç†
        data = self._preprocess_data(data)
        
        # æ·»åŠ æ‚£è€…ä¿¡æ¯
        data['patient_info'] = self.patient_info.get(patient_id, {})
        
        # æ·»åŠ ç–¾ç—…æ ‡ç­¾
        disease = data['patient_info'].get('Group', 'NOR')
        data['disease_label'] = self.DISEASE_MAPPING.get(disease, 0)
        
        # ç¼“å­˜æ•°æ®
        if self._cache is not None:
            self._cache[patient_id] = data
        
        return data
    
    def _preprocess_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ•°æ®é¢„å¤„ç†"""
        # é‡é‡‡æ ·
        if self.target_spacing is not None and 'metadata' in data:
            original_spacing = data['metadata']['spacing']
            
            for key in ['image', 'images', 'segmentation', 'segmentations']:
                if key in data and data[key] is not None:
                    if key in ['image', 'segmentation']:
                        # å•ä¸ªå›¾åƒ
                        data[key] = self._resample_image(
                            data[key], original_spacing, self.target_spacing
                        )
                    else:
                        # å›¾åƒåºåˆ—
                        resampled = []
                        for img in data[key]:
                            resampled.append(self._resample_image(
                                img, original_spacing, self.target_spacing
                            ))
                        data[key] = np.stack(resampled)
        
        # å¼ºåº¦å½’ä¸€åŒ–
        if self.normalize:
            for key in ['image', 'images']:
                if key in data and data[key] is not None:
                    if key == 'image':
                        data[key] = self._normalize_intensity(data[key])
                    else:
                        normalized = []
                        for img in data[key]:
                            normalized.append(self._normalize_intensity(img))
                        data[key] = np.stack(normalized)
        
        return data
    
    def __len__(self) -> int:
        return len(self.patient_list)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """è·å–å•ä¸ªæ ·æœ¬"""
        patient_id = self.patient_list[idx]
        data = self._load_patient_data(patient_id)
        
        # åº”ç”¨æ•°æ®å˜æ¢
        if self.transform is not None:
            data = self.transform(data)
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        for key in ['image', 'images', 'segmentation', 'segmentations']:
            if key in data and data[key] is not None:
                data[key] = torch.from_numpy(data[key]).float()
                if key in ['segmentation', 'segmentations']:
                    data[key] = data[key].long()
        
        data['disease_label'] = torch.tensor(data['disease_label'], dtype=torch.long)
        
        return data
    
    def get_patient_info(self, patient_id: str) -> Dict:
        """è·å–æ‚£è€…ä¿¡æ¯"""
        return self.patient_info.get(patient_id, {})
    
    def get_disease_distribution(self) -> Dict[str, int]:
        """è·å–ç–¾ç—…åˆ†å¸ƒç»Ÿè®¡"""
        distribution = {}
        for patient_id in self.patient_list:
            info = self.patient_info.get(patient_id, {})
            disease = info.get('Group', 'NOR')
            distribution[disease] = distribution.get(disease, 0) + 1
        return distribution


class ACDCDataModule:
    """
    ACDCæ•°æ®æ¨¡å—ï¼Œæä¾›ä¾¿æ·çš„æ•°æ®åŠ è½½æ¥å£
    """
    
    def __init__(
        self,
        data_root: Union[str, Path],
        batch_size: int = 4,
        num_workers: int = 4,
        mode: str = '3d_keyframes',
        target_spacing: Optional[Tuple[float, ...]] = None,
        normalize: bool = True,
        cache_data: bool = False
    ):
        self.data_root = data_root
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.mode = mode
        self.target_spacing = target_spacing
        self.normalize = normalize
        self.cache_data = cache_data
        
        self.train_dataset = None
        self.test_dataset = None
    
    def setup(self):
        """è®¾ç½®æ•°æ®é›†"""
        # è®­ç»ƒé›†
        self.train_dataset = ACDCDataset(
            data_root=self.data_root,
            split='training',
            mode=self.mode,
            target_spacing=self.target_spacing,
            normalize=self.normalize,
            cache_data=self.cache_data
        )
        
        # æµ‹è¯•é›†
        self.test_dataset = ACDCDataset(
            data_root=self.data_root,
            split='testing',
            mode=self.mode,
            load_segmentation=False,  # æµ‹è¯•é›†é€šå¸¸æ²¡æœ‰æ ‡æ³¨
            target_spacing=self.target_spacing,
            normalize=self.normalize,
            cache_data=self.cache_data
        )
    
    def train_dataloader(self) -> DataLoader:
        """è®­ç»ƒæ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True
        )
    
    def test_dataloader(self) -> DataLoader:
        """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True
        )
    
    def get_statistics(self) -> Dict:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        if self.train_dataset is None:
            self.setup()
        
        stats = {
            'train_size': len(self.train_dataset),
            'test_size': len(self.test_dataset),
            'disease_distribution': self.train_dataset.get_disease_distribution(),
            'mode': self.mode,
            'target_spacing': self.target_spacing
        }
        
        return stats


def collate_fn(batch: List[Dict]) -> Dict[str, Any]:
    """
    è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°ï¼Œå¤„ç†ä¸åŒå¤§å°çš„å›¾åƒ
    """
    # æå–æ‰€æœ‰é”®
    keys = batch[0].keys()
    collated = {}
    
    for key in keys:
        if key in ['image', 'images', 'segmentation', 'segmentations']:
            # å›¾åƒæ•°æ®éœ€è¦ç‰¹æ®Šå¤„ç†
            items = [item[key] for item in batch if item[key] is not None]
            if items:
                collated[key] = torch.stack(items)
            else:
                collated[key] = None
        elif key == 'disease_label':
            collated[key] = torch.tensor([item[key] for item in batch])
        else:
            collated[key] = [item[key] for item in batch]
    
    return collated


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    data_root = "/Users/fenghaojie/Documents/ICLR/MedCompression/acdc_dataset"
    
    # åˆ›å»ºæ•°æ®æ¨¡å—
    data_module = ACDCDataModule(
        data_root=data_root,
        batch_size=2,
        mode='3d_keyframes',
        target_spacing=(1.5, 1.5, 10.0),
        normalize=True
    )
    
    # è®¾ç½®æ•°æ®é›†
    data_module.setup()
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = data_module.get_statistics()
    print("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # è·å–æ•°æ®åŠ è½½å™¨
    train_loader = data_module.train_dataloader()
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    print("\nğŸ” æµ‹è¯•æ•°æ®åŠ è½½:")
    for i, batch in enumerate(train_loader):
        print(f"Batch {i}:")
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape} ({value.dtype})")
            else:
                print(f"  {key}: {type(value)}")
        
        if i >= 2:  # åªæµ‹è¯•å‰3ä¸ªbatch
            break
