#!/usr/bin/env python3
"""
TiTok Fine-tuning Script for ACDC MRI Dataset

å¯¹TiTokæ¨¡å‹åœ¨ACDCå¿ƒè„MRIæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒ
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import numpy as np
from PIL import Image
import argparse
import yaml
from datetime import datetime
import logging
from tqdm import tqdm
import warnings
from typing import Tuple, Dict, Any, Optional
from omegaconf import OmegaConf
import csv

# Manual implementations of image quality metrics
def compute_ssim(img1, img2, data_range=1.0, win_size=11, sigma=1.5):
    """Compute SSIM manually using torch operations"""
    try:
        # Create Gaussian kernel
        coords = torch.arange(win_size, dtype=torch.float32, device=img1.device)
        coords -= win_size // 2

        g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))
        g /= g.sum()

        kernel = g[:, None] * g[None, :]
        kernel = kernel.expand(img1.shape[1], 1, win_size, win_size).contiguous()

        # Compute local means
        mu1 = torch.nn.functional.conv2d(img1, kernel, groups=img1.shape[1], padding=win_size//2)
        mu2 = torch.nn.functional.conv2d(img2, kernel, groups=img2.shape[1], padding=win_size//2)

        mu1_sq = mu1 ** 2
        mu2_sq = mu2 ** 2
        mu1_mu2 = mu1 * mu2

        # Compute local variances and covariance
        sigma1_sq = torch.nn.functional.conv2d(img1 ** 2, kernel, groups=img1.shape[1], padding=win_size//2) - mu1_sq
        sigma2_sq = torch.nn.functional.conv2d(img2 ** 2, kernel, groups=img2.shape[1], padding=win_size//2) - mu2_sq
        sigma12 = torch.nn.functional.conv2d(img1 * img2, kernel, groups=img1.shape[1], padding=win_size//2) - mu1_mu2

        # Constants
        C1 = (0.01 * data_range) ** 2
        C2 = (0.03 * data_range) ** 2

        # Compute SSIM
        numerator = (2 * mu1_mu2 + C1) * (2 * sigma12 + C2)
        denominator = (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)

        ssim_map = numerator / denominator
        return ssim_map.mean().item()

    except Exception as e:
        print(f"âš ï¸ Error computing SSIM: {e}")
        return None


# Try to import torchmetrics, fallback to manual implementations
try:
    from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM
    from torchmetrics.image import PeakSignalNoiseRatio as PSNR
    from torchmetrics.image import LearnedPerceptualImagePatchSimilarity as LPIPS
    HAS_TORCHMETRICS = True
    print("âœ… torchmetrics available - using optimized implementations")
except ImportError:
    print("âš ï¸ torchmetrics not available - using manual implementations for SSIM")
    print("ğŸ’¡ SSIM will be computed manually, LPIPS will be skipped")
    HAS_TORCHMETRICS = False

# æ·»åŠ é¡¹ç›®è·¯å¾„ - æ”¯æŒä»med_mriç›®å½•æˆ–MedCompressionç›®å½•è¿è¡Œ
current_dir = Path(__file__).parent
med_root = current_dir if current_dir.name == 'med_mri' else Path.cwd()
med_compression_root = med_root.parent if med_root.name == 'med_mri' else med_root

sys.path.insert(0, str(med_compression_root))
sys.path.insert(0, str(med_root))

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
try:
    from modeling.titok import TiTok
    from modeling.maskgit import ImageBert
except ImportError:
    # å¦‚æœåœ¨1d-tokenizerä¸­è¿è¡Œï¼Œè°ƒæ•´è·¯å¾„
    sys.path.insert(0, str(med_compression_root / '1d-tokenizer'))
    from modeling.titok import TiTok
    from modeling.maskgit import ImageBert

try:
    from med_mri.acdc_dataset import create_data_loaders, ACDCMRIDataset
except ImportError:
    from acdc_dataset import create_data_loaders, ACDCMRIDataset

# å¯¼å…¥æŸå¤±å‡½æ•°
try:
    from med_mri.loss import SimpleMRILoss, TiTokMRILoss
except ImportError:
    from loss import SimpleMRILoss, TiTokMRILoss

warnings.filterwarnings("ignore")


class TiTokMRIWrapper(nn.Module):
    """
    TiTokæ¨¡å‹åŒ…è£…å™¨ï¼Œç”¨äºMRIå¾®è°ƒ
    """

    def __init__(self, tokenizer_path: str, generator_path: str = None, device: str = 'cuda'):
        """
        åˆå§‹åŒ–TiTok MRIåŒ…è£…å™¨

        Args:
            tokenizer_path: tokenizer checkpointè·¯å¾„
            generator_path: generator checkpointè·¯å¾„ (å¯é€‰)
            device: è®¡ç®—è®¾å¤‡
        """
        super().__init__()

        self.device = device
        self.tokenizer_path = tokenizer_path
        self.generator_path = generator_path

        # åŠ è½½tokenizer
        self.tokenizer = self._load_tokenizer(tokenizer_path)
        self.tokenizer.eval()

        # åŠ è½½generator (å¦‚æœæä¾›)
        if generator_path and os.path.exists(generator_path):
            self.generator = self._load_generator(generator_path)
            self.generator.eval()
        else:
            self.generator = None

        print(f"âœ… TiTokæ¨¡å‹åŠ è½½å®Œæˆ - Tokenizer: {tokenizer_path}")
        if self.generator:
            print(f"âœ… Generator: {generator_path}")

    def _load_tokenizer(self, checkpoint_path: str) -> TiTok:
        """åŠ è½½TiTok tokenizer"""
        checkpoint_path_obj = Path(checkpoint_path)

        # Check if local checkpoint directory exists and has model files
        if checkpoint_path_obj.exists() and checkpoint_path_obj.is_dir():
            model_files = list(checkpoint_path_obj.glob("*.safetensors")) + list(checkpoint_path_obj.glob("*.bin"))
            if model_files:
                print(f"âœ… ä»æœ¬åœ°è·¯å¾„åŠ è½½tokenizer: {checkpoint_path}")
                tokenizer = TiTok.from_pretrained(checkpoint_path)
                return tokenizer

        # Fallback to HuggingFace loading
        model_name = self._get_model_name_from_path(checkpoint_path)
        try:
            tokenizer = TiTok.from_pretrained(model_name)
            print(f"âœ… ä»HuggingFaceåŠ è½½tokenizer: {model_name}")
        except Exception as e:
            print(f"âš ï¸ HuggingFaceåŠ è½½å¤±è´¥: {e}")
            print(f"ğŸ’¡ è¯·ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½åˆ°: {checkpoint_path}")
            print(f"   æˆ–è¿è¡Œ: python download_checkpoints.py --best-only")
            raise e

        return tokenizer

    def _load_generator(self, checkpoint_path: str) -> ImageBert:
        """åŠ è½½generator"""
        model_name = self._get_model_name_from_path(checkpoint_path)
        try:
            generator = ImageBert.from_pretrained(model_name)
            print(f"âœ… ä»HuggingFaceåŠ è½½generator: {model_name}")
        except Exception as e:
            print(f"âš ï¸ GeneratoråŠ è½½å¤±è´¥: {e}")
            return None

        return generator

    def _get_model_name_from_path(self, path: str) -> str:
        """ä»è·¯å¾„æå–æ¨¡å‹åç§°"""
        path_lower = path.lower()

        # Handle direct model names
        if path.startswith('yucornetto/'):
            return path

        # Handle local directory names
        if 'tokenizer_titok_bl128_vae_c16_imagenet' in path_lower:
            return 'yucornetto/tokenizer_titok_bl128_vae_c16_imagenet'
        elif 'tokenizer_titok_b64_imagenet' in path_lower:
            return 'yucornetto/tokenizer_titok_b64_imagenet'
        elif 'generator_titok_b64_imagenet' in path_lower:
            return 'yucornetto/generator_titok_b64_imagenet'
        elif 'b64' in path_lower:
            return 'yucornetto/tokenizer_titok_b64_imagenet'
        elif 'b128' in path_lower:
            return 'yucornetto/tokenizer_titok_b128_imagenet'

        return 'yucornetto/tokenizer_titok_b64_imagenet'

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­: ç¼–ç  -> è§£ç """
        # ç¼–ç ä¸ºtoken (returns (tokens, info_dict))
        encode_result = self.tokenizer.encode(x)
        if isinstance(encode_result, tuple):
            tokens, _ = encode_result  # tokens shape: [B, C, 1, 64]
        else:
            tokens = encode_result

        # è§£ç é‡å»ºå›¾åƒ
        if self.generator is not None:
            reconstructed = self.generator.decode(tokens)
        else:
            # ä½¿ç”¨tokenizerçš„è§£ç å™¨
            decode_result = self.tokenizer.decode(tokens)
            # Handle both tensor and tuple returns
            if isinstance(decode_result, tuple):
                reconstructed = decode_result[0]
            else:
                reconstructed = decode_result

        return reconstructed, tokens

    def tokenize(self, x: torch.Tensor) -> torch.Tensor:
        """ä»…ç¼–ç """
        with torch.no_grad():
            tokens = self.tokenizer.encode(x)
        return tokens

    def to(self, device):
        """ç§»åŠ¨åˆ°è®¾å¤‡"""
        self.device = device
        self.tokenizer = self.tokenizer.to(device)
        if self.generator is not None:
            self.generator = self.generator.to(device)
        return super().to(device)


class TiTokMRIEvaluator:
    """è¯„ä¼°å™¨"""

    def __init__(self, device='cuda'):
        self.device = device
        if HAS_TORCHMETRICS:
            # Initialize metrics
            self.ssim = SSIM(data_range=1.0).to(device)
            self.psnr_metric = PSNR(data_range=1.0).to(device)
            self.lpips = LPIPS(net_type='alex').to(device)  # Using AlexNet backbone
        else:
            self.ssim = None
            self.psnr_metric = None
            self.lpips = None

    def compute_metrics(self, images, reconstructed, tokens):
        """è®¡ç®—é‡å»ºæŒ‡æ ‡"""
        # MSE
        mse = torch.mean((images - reconstructed) ** 2).item()

        # PSNR (manual calculation)
        max_val = 1.0
        psnr = 20 * np.log10(max_val / np.sqrt(mse)) if mse > 0 else 100.0

        # å‹ç¼©ç‡
        original_pixels = images.shape[-2] * images.shape[-1] * images.shape[-3]
        n_tokens = tokens.shape[-1]
        compression_ratio = original_pixels / n_tokens

        metrics = {
            'mse': mse,
            'psnr': psnr,
            'compression_ratio': compression_ratio
        }

        # Additional metrics
        if HAS_TORCHMETRICS and self.ssim is not None:
            try:
                # SSIM (torchmetrics)
                ssim_val = self.ssim(reconstructed, images).item()
                metrics['ssim'] = ssim_val

                # PSNR (torchmetrics version)
                psnr_val = self.psnr_metric(reconstructed, images).item()
                metrics['psnr_torchmetrics'] = psnr_val

                # LPIPS
                lpips_val = self.lpips(reconstructed, images).mean().item()
                metrics['lpips'] = lpips_val

            except Exception as e:
                print(f"âš ï¸ Error computing torchmetrics: {e}")
        else:
            # Manual SSIM implementation when torchmetrics not available
            try:
                ssim_val = compute_ssim(reconstructed, images, data_range=1.0)
                if ssim_val is not None:
                    metrics['ssim'] = ssim_val
                    print(f"âœ… SSIM computed manually: {ssim_val:.4f}")
                else:
                    print("âš ï¸ Failed to compute SSIM manually")
            except Exception as e:
                print(f"âš ï¸ Error computing manual SSIM: {e}")

            print("ğŸ’¡ LPIPS requires torchmetrics - skipping for now")

        return metrics


def save_sample_images(images, reconstructed, save_dir, epoch, prefix="val", max_samples=8):
    """
    ä¿å­˜è¾“å…¥å’Œé‡å»ºå›¾åƒæ ·æœ¬åˆ°epochå­æ–‡ä»¶å¤¹

    Args:
        images: åŸå§‹å›¾åƒ [B, C, H, W]
        reconstructed: é‡å»ºå›¾åƒ [B, C, H, W]
        save_dir: åŸºç¡€ä¿å­˜ç›®å½•
        epoch: å½“å‰epoch
        prefix: æ–‡ä»¶åå‰ç¼€ ("val" æˆ– "test")
        max_samples: æœ€å¤§ä¿å­˜æ ·æœ¬æ•°
    """
    # åˆ›å»ºepochå­æ–‡ä»¶å¤¹
    epoch_dir = Path(save_dir) / f"epoch_{epoch:03d}"
    epoch_dir.mkdir(parents=True, exist_ok=True)

    # é™åˆ¶æ ·æœ¬æ•°é‡
    n_samples = min(max_samples, images.shape[0])

    for i in range(n_samples):
        # è½¬æ¢tensoråˆ°PILå›¾åƒ
        # è¾“å…¥å›¾åƒ (å‡è®¾æ˜¯RGBæ ¼å¼ï¼ŒèŒƒå›´[0,1])
        input_img = images[i].cpu().permute(1, 2, 0).numpy()  # [H, W, C]
        input_img = (input_img * 255).astype(np.uint8)
        input_pil = Image.fromarray(input_img)

        # é‡å»ºå›¾åƒ
        recon_img = reconstructed[i].cpu().permute(1, 2, 0).numpy()  # [H, W, C]
        recon_img = np.clip(recon_img, 0, 1)  # ç¡®ä¿èŒƒå›´åœ¨[0,1]
        recon_img = (recon_img * 255).astype(np.uint8)
        recon_pil = Image.fromarray(recon_img)

        # ä¿å­˜å›¾åƒåˆ°epochæ–‡ä»¶å¤¹
        input_path = epoch_dir / f"{prefix}_sample_{i:02d}_input.png"
        recon_path = epoch_dir / f"{prefix}_sample_{i:02d}_recon.png"

        input_pil.save(input_path)
        recon_pil.save(recon_path)


def init_metrics_csv(csv_path):
    """åˆå§‹åŒ–metrics CSVæ–‡ä»¶"""
    with open(csv_path, 'w', newline='') as f:
        writer = csv.writer(f)
        # å†™å…¥è¡¨å¤´ - æ‰©å±•æ”¯æŒæ–°çš„æŸå¤±ç»„ä»¶
        headers = [
            'epoch', 'phase', 'total_loss', 'reconstruction_loss', 'perceptual_loss',
            'weighted_gan_loss', 'discriminator_loss', 'logits_real', 'logits_fake',
            'mse', 'psnr', 'compression_ratio', 'ssim'
        ]
        if HAS_TORCHMETRICS:
            headers.extend(['psnr_torchmetrics', 'lpips'])
        writer.writerow(headers)


def log_metrics_to_csv(csv_path, epoch, phase, metrics):
    """å°†metricsè®°å½•åˆ°CSVæ–‡ä»¶"""
    with open(csv_path, 'a', newline='') as f:
        writer = csv.writer(f)

        # Handle different key names for different phases
        if phase == 'train':
            # Training has loss components but no validation metrics
            total_loss = metrics.get('total_loss', '')
            reconstruction_loss = metrics.get('reconstruction_loss', '')
            perceptual_loss = metrics.get('perceptual_loss', '')
            weighted_gan_loss = metrics.get('weighted_gan_loss', '')
            discriminator_loss = metrics.get('discriminator_loss', '')
            logits_real = metrics.get('logits_real', '')
            logits_fake = metrics.get('logits_fake', '')
            mse = psnr = compression_ratio = ssim = ''
        else:
            # Validation/Test have evaluation metrics
            total_loss = reconstruction_loss = perceptual_loss = weighted_gan_loss = ''
            discriminator_loss = logits_real = logits_fake = ''
            mse = metrics.get('val_mse', metrics.get('mse', ''))
            psnr = metrics.get('val_psnr', metrics.get('psnr', ''))
            compression_ratio = metrics.get('compression_ratio', '')
            ssim = metrics.get('ssim', '')

        row = [
            epoch, phase, total_loss, reconstruction_loss, perceptual_loss,
            weighted_gan_loss, discriminator_loss, logits_real, logits_fake,
            mse, psnr, compression_ratio, ssim
        ]

        if HAS_TORCHMETRICS:
            row.extend([metrics.get('psnr_torchmetrics', ''), metrics.get('lpips', '')])

        writer.writerow(row)


def train_one_epoch(
    model: TiTokMRIWrapper,
    train_loader: DataLoader,
    optimizer: optim.Optimizer,
    loss_fn: nn.Module,
    device: str,
    epoch: int,
    total_epochs: int,
    global_step: int = 0,
    discriminator_optimizer: optim.Optimizer = None,
) -> Dict[str, float]:
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()  # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
    model.tokenizer.train()  # ç¡®ä¿tokenizerå¤„äºè®­ç»ƒæ¨¡å¼

    total_metrics = {}
    n_batches = 0

    pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{total_epochs} [Train]")

    for batch in pbar:
        images = batch['image'].to(device)

        # å‰å‘ä¼ æ’­
        reconstructed, tokens = model(images)

        # è®¡ç®—ç”Ÿæˆå™¨æŸå¤± (å§‹ç»ˆè®­ç»ƒ)
        loss, loss_dict = loss_fn(images, reconstructed, global_step, mode="generator")

        # åå‘ä¼ æ’­ (ç”Ÿæˆå™¨)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ç´¯ç§¯ç”Ÿæˆå™¨metrics
        for key, value in loss_dict.items():
            if key not in total_metrics:
                total_metrics[key] = 0.0
            total_metrics[key] += value.item()

        # è®­ç»ƒåˆ¤åˆ«å™¨ (å¦‚æœå¯ç”¨ä¸”åˆ°è¾¾è®­ç»ƒæ­¥æ•°)
        if discriminator_optimizer is not None and hasattr(loss_fn, 'should_discriminator_be_trained') and loss_fn.should_discriminator_be_trained(global_step):
            # åˆ¤åˆ«å™¨è®­ç»ƒæ­¥
            discriminator_loss, discriminator_loss_dict = loss_fn(images, reconstructed, global_step, mode="discriminator")

            # åå‘ä¼ æ’­ (åˆ¤åˆ«å™¨)
            discriminator_optimizer.zero_grad()
            discriminator_loss.backward()
            discriminator_optimizer.step()

            # ç´¯ç§¯åˆ¤åˆ«å™¨metrics
            for key, value in discriminator_loss_dict.items():
                metric_key = f"discriminator_{key}"
                if metric_key not in total_metrics:
                    total_metrics[metric_key] = 0.0
                total_metrics[metric_key] += value.item() if torch.is_tensor(value) else value

        n_batches += 1
        global_step += 1

        # æ›´æ–°è¿›åº¦æ¡
        pbar_metrics = {'total_loss': total_metrics['total_loss'] / n_batches}
        if 'reconstruction_loss' in total_metrics:
            pbar_metrics['recon_loss'] = total_metrics['reconstruction_loss'] / n_batches
        if 'discriminator_loss' in total_metrics:
            pbar_metrics['disc_loss'] = total_metrics['discriminator_loss'] / n_batches
        pbar.set_postfix(pbar_metrics)

    # è®¡ç®—å¹³å‡å€¼
    avg_metrics = {key: value / n_batches for key, value in total_metrics.items()}
    return avg_metrics


def validate(
    model: TiTokMRIWrapper,
    val_loader: DataLoader,
    device: str,
    epoch: int,
    evaluator: TiTokMRIEvaluator,
    save_images: bool = False,
    save_dir: str = None,
    prefix: str = "val"
) -> Dict[str, float]:
    """éªŒè¯"""
    model.eval()
    total_mse = 0.0
    total_psnr = 0.0
    total_compression_ratio = 0.0
    total_ssim = 0.0
    total_psnr_torchmetrics = 0.0
    total_lpips = 0.0
    n_batches = 0

    with torch.no_grad():
        pbar = tqdm(val_loader, desc=f"Validating")
        for batch_idx, batch in enumerate(pbar):
            images = batch['image'].to(device)
            reconstructed, tokens = model(images)

            metrics = evaluator.compute_metrics(images, reconstructed, tokens)

            # Accumulate all metrics
            total_mse += metrics['mse']
            total_psnr += metrics['psnr']
            total_compression_ratio += metrics['compression_ratio']

            if 'ssim' in metrics:
                total_ssim += metrics['ssim']
            if 'psnr_torchmetrics' in metrics:
                total_psnr_torchmetrics += metrics['psnr_torchmetrics']
            if 'lpips' in metrics:
                total_lpips += metrics['lpips']

            n_batches += 1

            # ä¿å­˜ç¬¬ä¸€æ‰¹çš„å›¾åƒæ ·æœ¬
            if save_images and batch_idx == 0 and save_dir is not None:
                save_sample_images(images, reconstructed, save_dir, epoch, prefix)

            # Update progress bar with available metrics
            pbar_metrics = {'mse': total_mse / n_batches, 'psnr': total_psnr / n_batches}
            if 'ssim' in metrics:
                pbar_metrics['ssim'] = total_ssim / n_batches
            pbar.set_postfix(pbar_metrics)

    # Build return dictionary with all available metrics
    result = {
        'val_mse': total_mse / n_batches,
        'val_psnr': total_psnr / n_batches,
        'compression_ratio': total_compression_ratio / n_batches
    }

    # Add additional metrics if available
    if total_ssim > 0:
        result['ssim'] = total_ssim / n_batches
    if total_psnr_torchmetrics > 0:
        result['psnr_torchmetrics'] = total_psnr_torchmetrics / n_batches
    if total_lpips > 0:
        result['lpips'] = total_lpips / n_batches

    return result


def main(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)

    # è®¾å¤‡ - ä¼˜å…ˆä½¿ç”¨CUDA
    if args.device == 'cuda' and torch.cuda.is_available():
        device = 'cuda'
        logger.info(f"ä½¿ç”¨CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
    elif args.device == 'cpu':
        device = 'cpu'
        logger.info("ä½¿ç”¨CPUè®¾å¤‡")
    else:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"è‡ªåŠ¨é€‰æ‹©è®¾å¤‡: {device}")

    if device == 'cpu':
        logger.warning("âš ï¸ ä½¿ç”¨CPUè®­ç»ƒå°†éå¸¸æ…¢ï¼Œå»ºè®®ä½¿ç”¨CUDA GPU")

    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¿è¡Œç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"run_{timestamp}"

    # è¾“å‡ºç›®å½•ç»“æ„
    output_dir = Path(args.output_dir)
    run_dir = output_dir / 'checkpoints' / run_name
    run_dir.mkdir(parents=True, exist_ok=True)

    # å­ç›®å½•
    checkpoint_dir = run_dir  # checkpointsç›´æ¥ä¿å­˜åœ¨runç›®å½•ä¸‹
    images_base_dir = run_dir / 'images'
    logs_dir = run_dir / 'logs'
    logs_dir.mkdir(parents=True, exist_ok=True)

    # æ—¥å¿—æ–‡ä»¶
    metrics_log_path = logs_dir / 'training_metrics.csv'
    training_log_path = logs_dir / 'training.log'

    # è®¾ç½®æ—¥å¿—åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°
    file_handler = logging.FileHandler(training_log_path)
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    logger.addHandler(file_handler)

    logger.info(f"è¿è¡Œç›®å½•: {run_dir}")
    logger.info(f"æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {checkpoint_dir}")
    logger.info(f"æ—¥å¿—ä¿å­˜åˆ°: {logs_dir}")

    # å›¾åƒä¿å­˜ç›®å½• (æŒ‰epochç»„ç»‡)
    images_dir = images_base_dir if args.save_images else None
    if images_dir:
        logger.info(f"å›¾åƒå°†ä¿å­˜åˆ°: {images_dir} (æŒ‰epochç»„ç»‡)")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    logger.info("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    data_loaders = create_data_loaders(
        data_root=args.data_root,
        batch_size=args.batch_size,
        num_workers=4 if device == 'cuda' else 0,
        image_size=(256, 256),
        augment=True
    )

    # åˆ›å»ºæ¨¡å‹
    logger.info("åˆ›å»ºTiTokæ¨¡å‹...")
    model = TiTokMRIWrapper(
        tokenizer_path=args.tokenizer_path,
        generator_path=args.generator_path,
        device=device
    ).to(device)

    # å†»ç»“generatorï¼Œåªè®­ç»ƒtokenizer
    if model.generator is not None:
        for param in model.generator.parameters():
            param.requires_grad = False

    # ç¡®ä¿tokenizerå‚æ•°å¯è®­ç»ƒ
    for param in model.tokenizer.parameters():
        param.requires_grad = True

    # è°ƒè¯•ï¼šæ£€æŸ¥å¯è®­ç»ƒå‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")

    # åˆ›å»ºæŸå¤±å‡½æ•°
    logger.info("åˆ›å»ºæŸå¤±å‡½æ•°...")
    loss_config = {
        'reconstruction_weight': getattr(args, 'reconstruction_weight', 1.0),
        'perceptual_weight': getattr(args, 'perceptual_weight', 0.1),
        'reconstruction_loss_type': getattr(args, 'reconstruction_loss_type', 'l2'),
        'perceptual_net_type': getattr(args, 'perceptual_net_type', 'vgg16'),
        'use_gan': getattr(args, 'use_gan', False),
        'discriminator_weight': getattr(args, 'discriminator_weight', 0.5),
        'discriminator_start': getattr(args, 'discriminator_start', 1000),
        'discriminator_input_nc': 3,  # TiTokè¾“å‡ºRGBå›¾åƒ
        'discriminator_ndf': 32,
        'discriminator_layers': 3,
    }

    if getattr(args, 'use_full_loss', False):
        loss_fn = TiTokMRILoss(loss_config)
        logger.info("ä½¿ç”¨å®Œæ•´çš„TiTok MRIæŸå¤±å‡½æ•°")
    else:
        loss_fn = SimpleMRILoss(loss_config)
        logger.info("ä½¿ç”¨ç®€åŒ–çš„MRIæŸå¤±å‡½æ•° (é‡å»º + æ„ŸçŸ¥)")

    loss_fn = loss_fn.to(device)

    # åˆ›å»ºåˆ¤åˆ«å™¨ä¼˜åŒ–å™¨ (å¦‚æœä½¿ç”¨GAN)
    discriminator_optimizer = None
    if hasattr(loss_fn, 'discriminator') and loss_fn.use_gan:
        import torch.optim as optim
        discriminator_params = list(loss_fn.discriminator.parameters())
        discriminator_optimizer = optim.AdamW(discriminator_params, lr=1e-4, weight_decay=1e-4)
        logger.info("åˆ›å»ºåˆ¤åˆ«å™¨ä¼˜åŒ–å™¨")

    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=args.learning_rate,
        weight_decay=1e-4
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.num_epochs)

    # è¯„ä¼°å™¨
    evaluator = TiTokMRIEvaluator(device=device)

    # åˆå§‹åŒ–metrics CSV
    init_metrics_csv(metrics_log_path)
    logger.info(f"Metricså°†è®°å½•åˆ°: {metrics_log_path}")

    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    global_step = 0
    logger.info("å¼€å§‹è®­ç»ƒ...")

    for epoch in range(args.num_epochs):
        # è®­ç»ƒ
        train_metrics = train_one_epoch(
            model,
            data_loaders['train'],
            optimizer,
            loss_fn,
            device,
            epoch,
            args.num_epochs,
            global_step,
            discriminator_optimizer
        )

        # æ›´æ–°global_step
        global_step += len(data_loaders['train'])

        # éªŒè¯ - å®šæœŸä¿å­˜å›¾åƒæ ·æœ¬
        save_images_current = args.save_images and ((epoch + 1) % args.save_image_every == 0 or epoch == 0)
        val_metrics = validate(
            model,
            data_loaders['val'],
            device,
            epoch,
            evaluator,
            save_images=save_images_current,
            save_dir=str(images_dir) if images_dir else None,
            prefix="val"
        )

        scheduler.step()

        # è®°å½•metricsåˆ°CSV
        log_metrics_to_csv(metrics_log_path, epoch + 1, 'train', train_metrics)
        log_metrics_to_csv(metrics_log_path, epoch + 1, 'val', val_metrics)

        # æ—¥å¿— - ä½¿ç”¨æ–°çš„metricsç»“æ„
        train_loss_display = train_metrics.get('total_loss', train_metrics.get('train_loss', 0.0))
        log_msg = f"Epoch {epoch + 1}/{args.num_epochs} - Train Loss: {train_loss_display:.6f} - Val MSE: {val_metrics['val_mse']:.6f} - Val PSNR: {val_metrics['val_psnr']:.2f}"

        # æ·»åŠ è®­ç»ƒæŸå¤±ç»„ä»¶è¯¦æƒ…
        if 'reconstruction_loss' in train_metrics:
            log_msg += f" - Recon: {train_metrics['reconstruction_loss']:.6f}"
        if 'perceptual_loss' in train_metrics:
            log_msg += f" - Percep: {train_metrics['perceptual_loss']:.6f}"

        # æ·»åŠ éªŒè¯metricsè¯¦æƒ…
        if 'ssim' in val_metrics:
            log_msg += f" - Val SSIM: {val_metrics['ssim']:.4f}"
        if 'lpips' in val_metrics:
            log_msg += f" - Val LPIPS: {val_metrics['lpips']:.4f}"

        logger.info(log_msg)

        # ä¿å­˜checkpoint
        if (epoch + 1) % args.save_every == 0 or (epoch + 1) == args.num_epochs:
            checkpoint_path = checkpoint_dir / f'checkpoint_epoch_{epoch + 1:03d}.pt'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.tokenizer.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_metrics['total_loss'],
                'val_mse': val_metrics['val_mse'],
            }, checkpoint_path)
            logger.info(f"Checkpointä¿å­˜åˆ°: {checkpoint_path}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['val_mse'] < best_val_loss:
            best_val_loss = val_metrics['val_mse']
            best_path = checkpoint_dir / 'best_model.pt'
            torch.save(model.tokenizer.state_dict(), best_path)

    # æµ‹è¯•é›†è¯„ä¼°
    logger.info("åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    test_metrics = validate(
        model,
        data_loaders['test'],
        device,
        epoch=-1,  # ä½¿ç”¨-1è¡¨ç¤ºæµ‹è¯•
        evaluator=evaluator,
        save_images=args.save_images,
        save_dir=str(images_dir) if images_dir else None,
        prefix="test"
    )

    # è®°å½•æµ‹è¯•metricsåˆ°CSV
    log_metrics_to_csv(metrics_log_path, -1, 'test', test_metrics)

    # æµ‹è¯•ç»“æœæ—¥å¿—
    test_log_msg = f"æµ‹è¯•ç»“æœ - MSE: {test_metrics['val_mse']:.6f} - PSNR: {test_metrics['val_psnr']:.2f}"

    # æ·»åŠ é¢å¤–metricsåˆ°æµ‹è¯•æ—¥å¿—
    if 'ssim' in test_metrics:
        test_log_msg += f" - SSIM: {test_metrics['ssim']:.4f}"
    if 'lpips' in test_metrics:
        test_log_msg += f" - LPIPS: {test_metrics['lpips']:.4f}"

    logger.info(test_log_msg)
    logger.info("è®­ç»ƒå®Œæˆï¼")
    logger.info(f"æ‰€æœ‰è¾“å‡ºä¿å­˜åœ¨: {run_dir}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='TiTok MRI Fine-tuning')
    parser.add_argument('--data_root', type=str,
                      default='../acdc_img_datasets',
                      help='ACDCæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./outputs',
                      help='è¾“å‡ºç›®å½•')
    parser.add_argument('--tokenizer_path', type=str,
                      default='./checkpoints/tokenizer_titok_bl128_vae_c16_imagenet',
                      help='Tokenizer checkpointè·¯å¾„ (é»˜è®¤: æœ€ä½³æ€§èƒ½æ¨¡å‹)')
    parser.add_argument('--generator_path', type=str,
                      default=None,
                      help='Generator checkpointè·¯å¾„ (å¯é€‰)')
    parser.add_argument('--batch_size', type=int, default=8,
                      help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_epochs', type=int, default=20,
                      help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                      help='å­¦ä¹ ç‡')
    parser.add_argument('--save_every', type=int, default=5,
                      help='æ¯å¤šå°‘ä¸ªepochä¿å­˜ä¸€æ¬¡checkpoint')
    parser.add_argument('--save_images', action='store_true',
                      help='ä¿å­˜éªŒè¯å’Œæµ‹è¯•å›¾åƒæ ·æœ¬')
    parser.add_argument('--save_image_every', type=int, default=5,
                      help='æ¯å¤šå°‘ä¸ªepochä¿å­˜ä¸€æ¬¡éªŒè¯å›¾åƒ')
    parser.add_argument('--device', type=str, default='cuda',
                      help='è®¡ç®—è®¾å¤‡ (cuda/cpu/auto)')

    # æŸå¤±å‡½æ•°ç›¸å…³å‚æ•°
    parser.add_argument('--use_full_loss', action='store_true',
                      help='ä½¿ç”¨å®Œæ•´çš„TiTokæŸå¤±å‡½æ•° (åŒ…å«GANé€‰é¡¹ï¼Œé»˜è®¤ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬)')
    parser.add_argument('--reconstruction_weight', type=float, default=1.0,
                      help='é‡å»ºæŸå¤±æƒé‡')
    parser.add_argument('--perceptual_weight', type=float, default=0.1,
                      help='æ„ŸçŸ¥æŸå¤±æƒé‡')
    parser.add_argument('--reconstruction_loss_type', type=str, default='l2',
                      choices=['l1', 'l2'], help='é‡å»ºæŸå¤±ç±»å‹ (l1æˆ–l2)')
    parser.add_argument('--perceptual_net_type', type=str, default='vgg16',
                      choices=['vgg16', 'vgg19'], help='æ„ŸçŸ¥æŸå¤±ä½¿ç”¨çš„ç½‘ç»œç±»å‹')
    parser.add_argument('--use_gan', action='store_true',
                      help='å¯ç”¨GANå¯¹æŠ—è®­ç»ƒ (ä»…åœ¨ä½¿ç”¨å®Œæ•´æŸå¤±å‡½æ•°æ—¶æœ‰æ•ˆ)')
    parser.add_argument('--discriminator_weight', type=float, default=0.5,
                      help='åˆ¤åˆ«å™¨æŸå¤±æƒé‡')
    parser.add_argument('--discriminator_start', type=int, default=1000,
                      help='å¼€å§‹GANè®­ç»ƒçš„æ­¥æ•°')

    args = parser.parse_args()
    main(args)
