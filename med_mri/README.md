# ğŸ«€ TiTok MRI å¾®è°ƒ

åœ¨ACDCå¿ƒè„MRIæ•°æ®é›†ä¸Šå¾®è°ƒTiTokæ¨¡å‹ï¼Œç”¨äºåŒ»ç–—å›¾åƒå‹ç¼©ã€‚

**ç‰¹æ€§**ï¼š~3000xå‹ç¼©æ¯”ï¼ŒCPU/GPUæ”¯æŒï¼ŒPyTorchå®ç°ã€‚

## ğŸ“Š æ•°æ®é›†è®¾ç½®

### è½¬æ¢ACDCæ•°æ®é›† (NIfTI â†’ PNG)

å¦‚æœæ‚¨æœ‰åŸå§‹ACDCæ•°æ®é›†çš„NIfTIæ ¼å¼ï¼Œè¯·è½¬æ¢ä¸ºPNGå›¾åƒï¼š

```bash
# å¯¼èˆªåˆ°med_mriç›®å½•
cd med_mri

# è½¬æ¢NIfTIæ–‡ä»¶ä¸ºPNG (éœ€è¦nibabel)
python convert_acdc.py

# é¢„æœŸè¾“å‡ºç»“æ„ï¼š
# acdc_img_datasets/
# â”œâ”€â”€ patient001/
# â”‚   â”œâ”€â”€ frame_000_slice_000.png
# â”‚   â”œâ”€â”€ frame_000_slice_001.png
# â”‚   â””â”€â”€ ...
# â”œâ”€â”€ patient002/
# â”‚   â””â”€â”€ ...
# â””â”€â”€ ...
```

**è¦æ±‚**ï¼šå®‰è£…nibabelç”¨äºNIfTIæ–‡ä»¶å¤„ç†
```bash
pip install nibabel
```

**æ¥æº**ï¼šå°†åŸå§‹ACDCæ•°æ®é›†æ”¾ç½®åœ¨ `./acdc_dataset/` (ç›¸å¯¹äºmed_mriç›®å½•)

### é¢„è½¬æ¢æ•°æ®é›†

ç¡®ä¿ACDCæ•°æ®é›†å¯ç”¨ï¼š
```
../acdc_img_datasets/
```

## ğŸš€ å‘½ä»¤

### å®‰è£…ä¾èµ–
```bash
cd /userhome/cs3/fung0311/CVPR-2025/MedCompression
pip install torch torchvision transformers pillow numpy matplotlib tqdm pyyaml omegaconf
```

### ä¸‹è½½æ£€æŸ¥ç‚¹
```bash
# ğŸš€ è‡ªåŠ¨ä¸‹è½½æœ€ä½³æ€§èƒ½æ¨¡å‹ (H800æ¨è)
python download_checkpoints.py --best-only

# äº¤äº’å¼æ¨¡å‹é€‰æ‹© (æ˜¾ç¤ºå¸¦æœ‰æ¨èçš„èœå•)
python download_checkpoints.py

# ä¸‹è½½æœ€ä½³æ€§èƒ½æ¨¡å‹ (éäº¤äº’å¼)
python download_checkpoints.py --non-interactive

# ä¸‹è½½åˆ°è‡ªå®šä¹‰ç›®å½•
python download_checkpoints.py --output_dir ../my_checkpoints --best-only

# ä¸‹è½½ç‰¹å®šæ¨¡å‹
python download_checkpoints.py --models tokenizer_bl128_vae  # æœ€ä½³FID (0.84)
python download_checkpoints.py --models tokenizer_b64        # å¹³è¡¡æ€§èƒ½
python download_checkpoints.py --models tokenizer_bl128_vq   # VQæ›¿ä»£æ–¹æ¡ˆ

# åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
python download_checkpoints.py --list

**âš ï¸ éœ€è¦è®¤è¯ï¼š**
```bash
# è¿™äº›TiTokæ¨¡å‹éœ€è¦HuggingFaceè®¤è¯
huggingface-cli login

# æˆ–è€…è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
# export HF_TOKEN=your_token_here
```

**æ‰‹åŠ¨ä¸‹è½½ï¼š** å¦‚æœè‡ªåŠ¨ä¸‹è½½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ä»ä»¥ä¸‹é“¾æ¥ä¸‹è½½ï¼š
- https://huggingface.co/yucornetto/tokenizer_titok_bl128_vae_c16_imagenet
- https://huggingface.co/yucornetto/tokenizer_titok_b64_imagenet
```

**æ¨¡å‹æ€§èƒ½ (FIDåˆ†æ•° - è¶Šä½è¶Šå¥½)ï¼š**
- `tokenizer_bl128_vae` - **FID: 0.84** â­ æœ€ä½³æ€§èƒ½ (H800é»˜è®¤)
- `tokenizer_sl256_vq` - FID: 1.03
- `tokenizer_bl128_vq` - FID: 1.49
- `tokenizer_b64` - FID: 1.70 (å¹³è¡¡å¤§å°/æ€§èƒ½)
- `tokenizer_bl64_vae` - FID: 1.25
- `tokenizer_ll32_vae` - FID: 1.61

**å¯ç”¨æ¨¡å‹ï¼š**
- **VAEæ¨¡å‹**ï¼šæœ€ä½³é‡å»ºè´¨é‡ï¼Œé€‚åˆH800å¾®è°ƒ â­
- **VQæ¨¡å‹**ï¼šè´¨é‡å’Œé€Ÿåº¦çš„è‰¯å¥½å¹³è¡¡
- **æ ‡å‡†æ¨¡å‹**ï¼šæ›´å°å°ºå¯¸ï¼Œæ›´å¿«è®­ç»ƒ

## ğŸ”„ TiTokç»„ä»¶è¯´æ˜

### **Tokenizer (å¿…éœ€)**
- **ç›®çš„**ï¼šå°†å›¾åƒå‹ç¼©ä¸ºç¦»æ•£token (256Ã—256å›¾åƒå‹ç¼©ä¸º32ä¸ªtoken)
- **ä½¿ç”¨æ–¹æ³•**ï¼š`tokenizer.encode(image)` â†’ tokens, `tokenizer.decode(tokens)` â†’ é‡å»ºå›¾åƒ
- **æˆ‘ä»¬çš„ç”¨é€”**ï¼šåŒ»ç–—å›¾åƒå‹ç¼©ç”¨äºMRIæ•°æ®

### **Generator (å¯é€‰)**
- **ç›®çš„**ï¼šä»å¤´ç”Ÿæˆæ–°å›¾åƒä½¿ç”¨å­¦ä¹ çš„tokenæ¨¡å¼
- **ä½¿ç”¨æ–¹æ³•**ï¼šåˆ›å»ºæ–°é¢–å›¾åƒï¼Œä¸æ˜¯é‡å»º
- **æˆ‘ä»¬çš„ç”¨é€”**ï¼šåŒ»ç–—å‹ç¼©ä»»åŠ¡ä¸éœ€è¦

### **ä¸ºä»€ä¹ˆä¸éœ€è¦Generator**
- âœ… **ä»…é‡å»º**ï¼šæˆ‘ä»¬åªéœ€è¦å‹ç¼©/è§£å‹
- âœ… **åŒ»ç–—é‡ç‚¹**ï¼šè¯Šæ–­/å‹ç¼©ä¸éœ€è¦ç”Ÿæˆ
- âœ… **è®­ç»ƒæ›´å¿«**ï¼šæ›´å°‘çš„å‚æ•°éœ€è¦è®­ç»ƒ
- âœ… **H800ä¼˜åŒ–**ï¼šæ›´å°‘çš„å†…å­˜ä½¿ç”¨

### æµ‹è¯•è®¾ç½®
```bash
cd med_mri
python test_finetune.py
```

### è®­ç»ƒæ¨¡å‹ (CUDA - æ¨è)
```bash
# åŸºç¡€CUDAè®­ç»ƒ (é»˜è®¤ç®€åŒ–æŸå¤±å‡½æ•°)
python finetune_titok_mri.py --batch_size 8 --num_epochs 20

# CUDAè®­ç»ƒå¹¶ä¿å­˜å›¾åƒ
python finetune_titok_mri.py --batch_size 8 --num_epochs 20 --save_images --save_image_every 5

# ğŸš€ å®Œæ•´ç¤ºä¾‹ - æ‰€æœ‰åŠŸèƒ½å¼€å¯ (æ¨èç”¨äºé«˜è´¨é‡é‡å»º)
python finetune_titok_mri.py \
  --data_root ../acdc_img_datasets \
  --output_dir ./outputs \
  --tokenizer_path ./checkpoints/tokenizer_titok_bl128_vae_c16_imagenet \
  --batch_size 8 \
  --num_epochs 50 \
  --learning_rate 1e-4 \
  --save_every 10 \
  --save_images \
  --save_image_every 5 \
  --device cuda \
  --use_full_loss \
  --use_gan \
  --reconstruction_weight 1.0 \
  --perceptual_weight 0.15 \
  --reconstruction_loss_type l2 \
  --perceptual_net_type vgg16 \
  --discriminator_weight 0.8 \
  --discriminator_start 5

# ğŸ¯ é«˜è´¨é‡é‡å»ºé…ç½® (å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡)
python finetune_titok_mri.py \
  --batch_size 16 \
  --num_epochs 30 \
  --perceptual_weight 0.2 \
  --reconstruction_loss_type l1 \
  --save_images \
  --save_image_every 10

# ğŸƒâ€â™‚ï¸ å¿«é€Ÿå®éªŒé…ç½® (ç”¨äºè°ƒè¯•)
python finetune_titok_mri.py \
  --batch_size 4 \
  --num_epochs 5 \
  --perceptual_weight 0.05 \
  --save_images \
  --save_image_every 1
```

### é€šè¿‡SLURMè®­ç»ƒæ¨¡å‹ (GPU)
```bash
./gpu_run.sh train
```

### è®­ç»ƒæ¨¡å‹ (CPU - æ…¢)
```bash
python finetune_titok_mri.py --batch_size 2 --num_epochs 5 --device cpu
```

### GPUäº¤äº’å¼ä¼šè¯
```bash
./gpu_run.sh bash
```

## âš™ï¸ é…ç½®

### ğŸ“‹ å®Œæ•´å‚æ•°åˆ—è¡¨

#### åŸºç¡€è®­ç»ƒå‚æ•°
- `--data_root`ï¼šACDCæ•°æ®é›†è·¯å¾„ (é»˜è®¤: `../acdc_img_datasets`)
- `--output_dir`ï¼šè¾“å‡ºç›®å½• (é»˜è®¤: `./outputs`)
- `--tokenizer_path`ï¼šTokenizer checkpointè·¯å¾„
- `--batch_size`ï¼šè®­ç»ƒæ‰¹æ¬¡å¤§å° (é»˜è®¤: 8ï¼Œå»ºè®®: 4-16)
- `--num_epochs`ï¼šè®­ç»ƒè½®æ•° (é»˜è®¤: 20ï¼Œå»ºè®®: 20-100)
- `--learning_rate`ï¼šå­¦ä¹ ç‡ (é»˜è®¤: 1e-4ï¼Œå»ºè®®: 1e-5 åˆ° 5e-4)
- `--device`ï¼šè®¡ç®—è®¾å¤‡ (`cuda`/`cpu`/autoï¼Œé»˜è®¤: `cuda`)

#### æ£€æŸ¥ç‚¹å’Œä¿å­˜å‚æ•°
- `--save_every`ï¼šæ¯Nä¸ªepochä¿å­˜checkpoint (é»˜è®¤: 5)
- `--save_images`ï¼šä¿å­˜éªŒè¯/æµ‹è¯•å›¾åƒæ ·æœ¬ (æ ‡å¿—)
- `--save_image_every`ï¼šæ¯Nä¸ªepochä¿å­˜éªŒè¯å›¾åƒ (é»˜è®¤: 5)

#### ğŸ¯ æŸå¤±å‡½æ•°å‚æ•°

##### ç®€åŒ–çš„MRIæŸå¤±å‡½æ•° (é»˜è®¤)
- `--reconstruction_weight`ï¼šé‡å»ºæŸå¤±æƒé‡ (é»˜è®¤: 1.0)
- `--perceptual_weight`ï¼šæ„ŸçŸ¥æŸå¤±æƒé‡ (é»˜è®¤: 0.1ï¼Œå»ºè®®: 0.05-0.2)
- `--reconstruction_loss_type`ï¼šé‡å»ºæŸå¤±ç±»å‹ (`l1`/`l2`ï¼Œé»˜è®¤: `l2`)
- `--perceptual_net_type`ï¼šæ„ŸçŸ¥æŸå¤±ç½‘ç»œ (`vgg16`/`vgg19`ï¼Œé»˜è®¤: `vgg16`)

##### å®Œæ•´çš„TiTokæŸå¤±å‡½æ•°
- `--use_full_loss`ï¼šå¯ç”¨å®Œæ•´çš„TiTokæŸå¤±å‡½æ•° (æ ‡å¿—)
- `--use_gan`ï¼šå¯ç”¨GANå¯¹æŠ—è®­ç»ƒ (æ ‡å¿—)
- `--discriminator_weight`ï¼šåˆ¤åˆ«å™¨æŸå¤±æƒé‡ (é»˜è®¤: 0.5ï¼Œå»ºè®®: 0.3-1.0)
- `--discriminator_start`ï¼šGANè®­ç»ƒå¼€å§‹æ­¥æ•° (é»˜è®¤: 1000ï¼Œå»ºè®®: 500-2000)

### ğŸ’¡ ä½¿ç”¨å»ºè®®

#### ğŸ¯ é«˜è´¨é‡é‡å»º (æ¨èç”¨äºæœ€ç»ˆæ¨¡å‹)
```bash
# ä½¿ç”¨å®Œæ•´æŸå¤±å‡½æ•°ï¼Œè¾ƒé«˜çš„æ„ŸçŸ¥æƒé‡
--use_full_loss --use_gan --perceptual_weight 0.15 --discriminator_weight 0.8
```

#### âš¡ å¿«é€Ÿè®­ç»ƒ (ç”¨äºå®éªŒå’Œè°ƒè¯•)
```bash
# ç®€åŒ–æŸå¤±ï¼Œè¾ƒå°çš„batch size
--batch_size 4 --num_epochs 10 --perceptual_weight 0.05
```

#### ğŸ¥ åŒ»å­¦åº”ç”¨ (å¹³è¡¡è´¨é‡å’Œç¨³å®šæ€§)
```bash
# L1æŸå¤±æ›´ç¨³å®šï¼Œé€‚ä¸­çš„æ„ŸçŸ¥æƒé‡
--reconstruction_loss_type l1 --perceptual_weight 0.1 --batch_size 8
```

#### ğŸš€ å¤§è§„æ¨¡è®­ç»ƒ (éœ€è¦å¤§é‡GPUèµ„æº)
```bash
# æ›´å¤§çš„batch sizeï¼Œæ›´é•¿çš„è®­ç»ƒ
--batch_size 32 --num_epochs 100 --perceptual_weight 0.2 --use_full_loss
```


## ğŸ“ è¾“å‡ºç›®å½•ç»“æ„

```
outputs/checkpoints/run_<YYYYMMDD_HHMMSS>/
â”œâ”€â”€ checkpoint_epoch_005.pt          # æ£€æŸ¥ç‚¹
â”œâ”€â”€ best_model.pt                    # æœ€ä½³æ¨¡å‹
â”œâ”€â”€ images/                          # å›¾åƒä¿å­˜
â”‚   â”œâ”€â”€ epoch_000/                   # ç¬¬0ä¸ªepoch
â”‚   â”‚   â”œâ”€â”€ val_sample_00_input.png
â”‚   â”‚   â”œâ”€â”€ val_sample_00_recon.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ epoch_005/                   # ç¬¬5ä¸ªepoch
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ epoch_010/                   # ç¬¬10ä¸ªepoch
â”‚       â””â”€â”€ ...
â””â”€â”€ logs/                            # æ—¥å¿—æ–‡ä»¶
    â”œâ”€â”€ training.log                 # è®­ç»ƒæ—¥å¿—
    â””â”€â”€ training_metrics.csv         # Metrics CSV
```

## ğŸ”§ åŸºæœ¬æ•…éšœæ’é™¤

**CUDAä¸å¯ç”¨**ï¼šä½¿ç”¨ `--device cpu`
```bash
python finetune_titok_mri.py --device cpu
```

**å†…å­˜ä¸è¶³**ï¼šå‡å°‘æ‰¹æ¬¡å¤§å°
```bash
python finetune_titok_mri.py --batch_size 2
```

**æ•°æ®é›†æœªæ‰¾åˆ°**ï¼šæ£€æŸ¥è·¯å¾„
```bash
ls ../acdc_img_datasets/
```
